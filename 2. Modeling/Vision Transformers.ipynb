{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OUYY6Do4jLGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69be9b5b-2108-4418-dbb8-3b007f419aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from google.colab import drive\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "drive.mount(\"/content/drive/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,imgSize,patchSize,numOfChannels=1,embeddingDimension=769):\n",
        "    super().__init__()\n",
        "    self.imgSize=imgSize\n",
        "    self.patchSize=patchSize\n",
        "    self.numOfPatches = (imgSize//patchSize)**2\n",
        "    self.projection = nn.Conv2d(numOfChannels,embeddingDimension,kernel_size=patchSize,stride=patchSize)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    input = self.projection(input) #(number of samples,embeding dimension, sqrt number of patches,sqrt number of patches )\n",
        "    input = input.flatten(2) #(number of samples,embeding dimension,number of patches)\n",
        "    input = input.transpose(1,2) #(number of samples,number of patches, embeding dimension)\n",
        "    return input"
      ],
      "metadata": {
        "id": "DQRWUprJjXmx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self,dimensions,numOfHeads=12,queryKeyValueBias=True,kvpDropoutProbability=0.,projectionDropoutProbability=0.):\n",
        "    super().__init__()\n",
        "    self.numOfHeads = numOfHeads\n",
        "    self.dimensions = dimensions\n",
        "    self.headDimension = dimensions//numOfHeads\n",
        "    self.normalizationFactor = self.headDimension** -0.5  # extremly large values to softmax -> small gradients\n",
        "    \n",
        "    self.queryKeyValue = nn.Linear(dimensions,dimensions*3,bias =queryKeyValueBias )\n",
        "    self.kvpDropout = nn.Dropout(kvpDropoutProbability)\n",
        "    self.projection = nn.Linear(dimensions,dimensions)\n",
        "    self.projectionDropout = nn.Dropout(projectionDropoutProbability)\n",
        "\n",
        "  def forward(self,input):\n",
        "    numOfSamples,numOfTokens,dimensions = input.shape\n",
        "    if dimensions != self.dimensions:\n",
        "      raise ValueError(\"Dimensions shape in Attention Module\")\n",
        "    queryKeysValues = self.queryKeyValue(input) #(number of samples, number of patches+1, dimensions *3)\n",
        "    queryKeysValues = queryKeysValues.reshape(numOfSamples,numOfTokens,3,self.numOfHeads,self.headDimensions)\n",
        "    queryKeysValues = queryKeysValues.permute(2,0,3,1,4) #(3,number of samples, number of heads,number of patches +1, head dimenson)\n",
        "    query,key,value = queryKeysValues[0],queryKeysValues[1],queryKeysValues[2]\n",
        "    keyTranspose = key.Transpose(-2,-1) #(number of samples, number of heads,head dimension,number of patches)\n",
        "    dotProduct = (query @ keyTranspose) * self.normalizationFactor # (number of samples,number of heads,number of patches +1, number of patches + 1)\n",
        "    attention = dotProduct.softmax(dim=-1) # (number of samples,number of heads,number of patches +1, number of patches + 1)\n",
        "    attention = self.kvpDropout(attention)\n",
        "\n",
        "    weightedAverage = attention @ value # (number of samples,number of heads,number of patches +1, head dimension)\n",
        "    weightedAverage = weightedAverage.Transpose(1,2) #(number of samples, number of patches + 1,number of heads,head dimension)\n",
        "    weightedAverage = weightedAverage.flatten(2) #(number of samples, number of patches +1, dimensions)\n",
        "    input = self.projection(weightedAverage)\n",
        "    input = self.projectionDropout(input)\n",
        "    return input\n",
        "     "
      ],
      "metadata": {
        "id": "cyz_t3pmkEAg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "   def __init__(self,inputFeatures,hiddenFeatures,outputFeatures,prob=0.):\n",
        "      super().__init__()\n",
        "      self.fullyConnected1= nn.Linear(inputFeatures,hiddenFeatures)\n",
        "      self.activateion = nn.GELU()\n",
        "      self.fulltConnected2= nn.Linear(hiddenFeatures,outputFeatures)\n",
        "      self.dropout = nn.Dropout(prob)\n",
        "   \n",
        "   def forward (self,input):\n",
        "     input = self.fullyConnected1(input)\n",
        "     input = self.activateion(input)\n",
        "     input = self.dropout(input)\n",
        "     input = self.fullyConnected2(input)\n",
        "     input = self.dropout(input)\n",
        "     return input"
      ],
      "metadata": {
        "id": "OsGLLl1sN3Ma"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,dimensions,numOfHeads,MLPRatio,QKVBias=True,p=0.,attentionProb=0.):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.normalization1 = nn.LayerNorm(dimensions,eps=1e-6)\n",
        "    self.attention = AttentionModule(dimensions,numOfHeads,QKVBias,attentionProb,p)\n",
        "    self.normalization2= nn.LayerNorm(dimensions,eps=1e-6)\n",
        "    hiddenFeatures = int(dimensions*MLPRatio)\n",
        "    self.mlp = MLP(dimensions,hiddenFeatures,dimensions)\n",
        "\n",
        "  def forward(self,input):\n",
        "    #residual block\n",
        "    input = input + self.attention(self.normalization1(input))\n",
        "    input = input + self.mlp(self.normalization2(input))\n",
        "    return input\n",
        "     "
      ],
      "metadata": {
        "id": "I8SfykXNQS5m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,imgSize,patchSize,numOfChannels,numOfClasses,embeddingDim,depth, numOfHeads,MLPRatio,QKVBias,p=0.,attentionProb=0.):\n",
        "    super(VisionTransformer,self).__init__()\n",
        "    self.patchEmbedding = PatchEmbedding(imgSize,patchSize,numOfChannels,embeddingDim)\n",
        "    self.classToken = nn.Parameter(torch.zeros(1,1,embeddingDim))\n",
        "    self.positionalEmbedding = nn.Parameter(torch.zeros(1,1+self.patchEmbedding.numOfPatches,embeddingDim))\n",
        "    self.positionDropout = nn.Dropout(p)\n",
        "    self.blocks = nn.ModuleList([TransformerBlock(embeddingDim,numOfHeads,MLPRatio,QKVBias,p,attentionProb) for ctr in range (depth)])\n",
        "    self.normalization = nn.LayerNorm(embeddingDim,eps= 1e-6)\n",
        "    self.head = nn.Linear(embeddingDim,numOfClasses)\n",
        " \n",
        "    def foward(self,input):\n",
        "      numOfSamples = input.shape[0]\n",
        "      input = self.patchEmbedding(input)\n",
        "      classTokens = self.classToken.expand(numOfSamples,-1,-1)\n",
        "      input = torch.cat((classTokens,input),dim=1)\n",
        "      input = input + self.positionalEmbedding\n",
        "      input = self.positionDropout(input)\n",
        "      for block in self.blocks:\n",
        "        input = block(input)\n",
        "        input = self.normalization(input)\n",
        "        finalClassTokens = input[:,0]\n",
        "        input = self.head(finalClassTokens)\n",
        "      return input"
      ],
      "metadata": {
        "id": "_oNZdeOxVAS8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(200,20,1,5,768,12,8,0.4,True,0.3,0.2)"
      ],
      "metadata": {
        "id": "LxWwZOptXKK0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossFunction = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n"
      ],
      "metadata": {
        "id": "buq7Ba2Ci3Ya"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainPath = \"/content/drive/MyDrive/Deep_learning/Identification Data\"\n",
        "classes = ['personA','personB','personC','personD','personE']\n",
        "trainSet = []\n",
        "testSet = []\n",
        "imgSize = 200\n",
        "for person in os.listdir(mainPath):\n",
        "   personPath = os.path.join(mainPath,person)\n",
        "   for dataDir in os.listdir(personPath):\n",
        "      imagesPath = os.path.join(personPath,dataDir)\n",
        "      for image in os.listdir(imagesPath):\n",
        "        imagePath = os.path.join(imagesPath,image)\n",
        "        try:\n",
        "           image = cv2.imread(imagePath,0)\n",
        "           image = cv2.resize(image,(imgSize,imgSize))\n",
        "           label = classes.index(person)\n",
        "           if dataDir == \"Train\":\n",
        "              trainSet.append([image,label])\n",
        "           else:\n",
        "             testSet.append([image,label])\n",
        "        except:\n",
        "            print(imagePath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jHaA2M_NpWe",
        "outputId": "2772b53e-4e30-4905-846c-cf3909e310ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/Identification Data/personD/Train/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personD/Test/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personC/Train/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personC/Test/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personA/Train/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personA/Test/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personE/Test/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personE/Train/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personB/Test/.ipynb_checkpoints\n",
            "/content/drive/MyDrive/Deep_learning/Identification Data/personB/Train/.ipynb_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(trainSet)\n",
        "random.shuffle(testSet)"
      ],
      "metadata": {
        "id": "WNm9yE0bPyNZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "ZGzAnlkNUeXH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveModel():\n",
        "    path = \"./myFirstModel.pth\"\n",
        "    torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "qbyYshWdUtzc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testAccuracy():\n",
        "    \n",
        "    model.eval()\n",
        "    accuracy = 0.0\n",
        "    total = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in testSet:\n",
        "            images, labels = data\n",
        "            # run the model on the test set to predict labels\n",
        "            outputs = model(images)\n",
        "            # the label with the highest energy will be our prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "    \n",
        "    # compute the accuracy over all test images\n",
        "    accuracy = (100 * accuracy / total)\n",
        "    return(accuracy)"
      ],
      "metadata": {
        "id": "aZ4X3L0kUmUh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs):\n",
        "    \n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    # Define your execution device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"The model will be running on\", device, \"device\")\n",
        "    # Convert model parameters and buffers to CPU or Cuda\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        for i, (images, labels) in enumerate(trainSet, 0):\n",
        "            # get the inputs\n",
        "            #images = Variable(images.to(device))\n",
        "            #labels = Variable(labels.to(device))\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # predict classes using images from the training set\n",
        "            outputs = model.forward(images)\n",
        "            # compute the loss based on model output and real labels\n",
        "            loss = lossFunction(outputs, labels)\n",
        "            # backpropagate the loss\n",
        "            loss.backward()\n",
        "            # adjust parameters based on the calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Let's print statistics for every 1,000 images\n",
        "            running_loss += loss.item()     # extract the loss value\n",
        "            if i % 1000 == 999:    \n",
        "                # print every 1000 (twice per epoch) \n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 1000))\n",
        "                # zero the loss\n",
        "                running_loss = 0.0\n",
        "        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n",
        "        accuracy = testAccuracy()\n",
        "        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
        "        \n",
        "        # we want to save the model if the accuracy is the best\n",
        "        if accuracy > best_accuracy:\n",
        "            saveModel()\n",
        "            best_accuracy = accuracy"
      ],
      "metadata": {
        "id": "0YjgNPcNT_eV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "dw-0eykKUusc",
        "outputId": "7621b148-b360-410a-c129-c0bfc75206d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model will be running on cuda:0 device\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-84557d09d340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d3f23aecf36e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# predict classes using images from the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# compute the loss based on model output and real labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [VisionTransformer] is missing the required \"forward\" function"
          ]
        }
      ]
    }
  ]
}