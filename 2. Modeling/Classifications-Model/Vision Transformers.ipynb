{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OUYY6Do4jLGE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,imgSize,patchSize,numOfChannels=1,embeddingDimension=769):\n",
        "    super().__init__()\n",
        "    self.imgSize=imgSize\n",
        "    self.patchSize=patchSize\n",
        "    self.numOfPatches = (imgSize//patchSize)**2\n",
        "    self.projection = nn.Conv2d(numOfChannels,embeddingDimension,kernel_size=patchSize,stride=patchSize)\n",
        "  \n",
        "  def forward(self,input):\n",
        "    input = self.projection(input) #(number of samples,embeding dimension, sqrt number of patches,sqrt number of patches )\n",
        "    input = input.flatten(2) #(number of samples,embeding dimension,number of patches)\n",
        "    input = input.transpose(1,2) #(number of samples,number of patches, embeding dimension)\n",
        "    return input"
      ],
      "metadata": {
        "id": "DQRWUprJjXmx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self,dimensions,numOfHeads=12,queryKeyValueBias=True,kvpDropoutProbability=0.,projectionDropoutProbability=0.):\n",
        "    super().__init__()\n",
        "    self.numOfHeads = numOfHeads\n",
        "    self.dimensions = dimensions\n",
        "    self.headDimension = dimensions//numOfHeads\n",
        "    self.normalizationFactor = self.headDimension** -0.5  # extremly large values to softmax -> small gradients\n",
        "    \n",
        "    self.queryKeyValue = nn.Linear(dimensions,dimensions*3,bias =queryKeyValueBias )\n",
        "    self.kvpDropout = nn.Dropout(kvpDropoutProbability)\n",
        "    self.projection = nn.Linear(dimensions,dimensions)\n",
        "    self.projectionDropout = nn.Dropout(projectionDropoutProbability)\n",
        "\n",
        "  def forward(self,input):\n",
        "    numOfSamples,numOfTokens,dimensions = input.shape\n",
        "    if dimensions != self.dimensions:\n",
        "      raise ValueError(\"Dimensions shape in Attention Module\")\n",
        "    queryKeysValues = self.queryKeyValue(input) #(number of samples, number of patches+1, dimensions *3)\n",
        "    queryKeysValues = queryKeysValues.reshape(numOfSamples,numOfTokens,3,self.numOfHeads,self.headDimensions)\n",
        "    queryKeysValues = queryKeysValues.permute(2,0,3,1,4) #(3,number of samples, number of heads,number of patches +1, head dimenson)\n",
        "    query,key,value = queryKeysValues[0],queryKeysValues[1],queryKeysValues[2]\n",
        "    keyTranspose = key.Transpose(-2,-1) #(number of samples, number of heads,head dimension,number of patches)\n",
        "    dotProduct = (query @ keyTranspose) * self.normalizationFactor # (number of samples,number of heads,number of patches +1, number of patches + 1)\n",
        "    attention = dotProduct.softmax(dim=-1) # (number of samples,number of heads,number of patches +1, number of patches + 1)\n",
        "    attention = self.kvpDropout(attention)\n",
        "\n",
        "    weightedAverage = attention @ value # (number of samples,number of heads,number of patches +1, head dimension)\n",
        "    weightedAverage = weightedAverage.Transpose(1,2) #(number of samples, number of patches + 1,number of heads,head dimension)\n",
        "    weightedAverage = weightedAverage.flatten(2) #(number of samples, number of patches +1, dimensions)\n",
        "    input = self.projection(weightedAverage)\n",
        "    input = self.projectionDropout(input)\n",
        "    return input\n",
        "     "
      ],
      "metadata": {
        "id": "cyz_t3pmkEAg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "   def __init__(self,inputFeatures,hiddenFeatures,outputFeatures,prob=0.):\n",
        "      super().__init__()\n",
        "      self.fullyConnected1= nn.Linear(inputFeatures,hiddenFeatures)\n",
        "      self.activateion = nn.GELU()\n",
        "      self.fulltConnected2= nn.Linear(hiddenFeatures,outputFeatures)\n",
        "      self.dropout = nn.Dropout(prob)\n",
        "   \n",
        "   def forward (self,input):\n",
        "     input = self.fullyConnected1(input)\n",
        "     input = self.activateion(input)\n",
        "     input = self.dropout(input)\n",
        "     input = self.fullyConnected2(input)\n",
        "     input = self.dropout(input)\n",
        "     return input"
      ],
      "metadata": {
        "id": "OsGLLl1sN3Ma"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,dimensions,numOfHeads,MLPRatio,QKVBias=True,p=0.,attentionProb=0.):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.normalization1 = nn.LayerNorm(dimensions,eps=1e-6)\n",
        "    self.attention = AttentionModule(dimensions,numOfHeads,QKVBias,attentionProb,p)\n",
        "    self.normalization2= nn.LayerNorm(dimensions,eps=1e-6)\n",
        "    hiddenFeatures = int(dimensions*MLPRatio)\n",
        "    self.mlp = MLP(dimensions,hiddenFeatures,dimensions)\n",
        "\n",
        "  def forward(self,input):\n",
        "    #residual block\n",
        "    input = input + self.attention(self.normalization1(input))\n",
        "    input = input + self.mlp(self.normalization2(input))\n",
        "    return input\n",
        "     "
      ],
      "metadata": {
        "id": "I8SfykXNQS5m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,imgSize,patchSize,numOfChannels,numOfClasses,embeddingDim,depth, numOfHeads,MLPRatio,QKVBias,p=0.,attentionProb=0.):\n",
        "    super(VisionTransformer,self).__init__()\n",
        "    self.patchEmbedding = PatchEmbedding(imgSize,patchSize,numOfChannels,embeddingDim)\n",
        "    self.classToken = nn.Parameter(torch.zeros(1,1,embeddingDim))\n",
        "    self.positionalEmbedding = nn.Parameter(torch.zeros(1,1+self.patchEmbedding.numOfPatches,embeddingDim))\n",
        "    self.positionDropout = nn.Dropout(p)\n",
        "    self.blocks = nn.ModuleList([TransformerBlock(embeddingDim,numOfHeads,MLPRatio,QKVBias,p,attentionProb) for ctr in range (depth)])\n",
        "    self.normalization = nn.LayerNorm(embeddingDim,eps= 1e-6)\n",
        "    self.head = nn.Linear(embeddingDim,numOfClasses)\n",
        " \n",
        "  def foward(self,input):\n",
        "    numOfSamples = input.shape[0]\n",
        "    input = self.patchEmbedding(input)\n",
        "    classTokens = self.classToken.expand(numOfSamples,-1,-1)\n",
        "    input = torch.cat((classTokens,input),dim=1)\n",
        "    input = input + self.positionalEmbedding\n",
        "    input = self.positionDropout(input)\n",
        "    for block in self.blocks:\n",
        "      input = block(input)\n",
        "    input = self.normalization(input)\n",
        "    finalClassTokens = input[:,0]\n",
        "    input = self.head(finalClassTokens)\n",
        "    return input"
      ],
      "metadata": {
        "id": "_oNZdeOxVAS8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visionTransformer = VisionTransformer(200,20,1,5,768,12,8,0.4,True,0.3,0.2)"
      ],
      "metadata": {
        "id": "LxWwZOptXKK0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossFunction = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(visionTransformer.parameters(),lr=0.001,weight_decay=0.0001)\n"
      ],
      "metadata": {
        "id": "buq7Ba2Ci3Ya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}