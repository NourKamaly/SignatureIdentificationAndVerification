{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea0IiGnZc7s6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import time\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import resnet\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras import backend, layers, metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_PW-1KTdAM7"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/')\n",
        "path = '/content/drive/MyDrive/cv_data/*'\n",
        "persons = glob.glob(path,)\n",
        "persons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dictionary carry key -> name of person and value -> list of images that bleong to this person\n",
        "\n",
        "\n",
        "x_train_real\n",
        "    \n",
        "     'personA' : [image1, image2, image3]\n",
        "      'personB' : [image1, image2, image3]\n",
        "      'perosnC' : [image1, image2, image3]\n",
        "\n",
        "\n",
        "x_train_forged\n",
        "  \n",
        "    'personA' : [image1, image2, image3]\n",
        "    'personB' : [image1, image2, image3]\n",
        "    'perosnC' : [image1, image2, image3]\n",
        "\n",
        "  \n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_bAPPbMKIcXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIH_wxj3gbs3"
      },
      "outputs": [],
      "source": [
        "x_train_real = {}\n",
        "x_train_forged= {}\n",
        "\n",
        "x_test_real = {}\n",
        "x_test_forged= {}\n",
        "\n",
        "for person in persons:\n",
        "  \n",
        "  per = person.split('/cv_data/')[1]\n",
        "  print( f'get Data of : {per}')\n",
        "  x_train_real[per] = []\n",
        "  x_train_forged[per] = []\n",
        "\n",
        "  x_test_real[per] = []\n",
        "  x_test_forged[per] = []\n",
        "  \n",
        "  train_image = glob.glob(f'{person}/Train/*.png')\n",
        "  train_label = glob.glob(f'{person}/Train/*.csv')[0]\n",
        "  df_train_label = pd.read_csv(train_label)\n",
        "\n",
        "\n",
        "  test_image = glob.glob(f'{person}/Test/*.png')\n",
        "  test_label = glob.glob(f'{person}/Test/*.csv')[0]\n",
        "  df_test_label = pd.read_csv(test_label)\n",
        " \n",
        "  ## read train image and divided into real and fake\n",
        "  for image in train_image:\n",
        "    name_of_image = image.split('/Train/')[1]\n",
        "    img = cv2.imread(image,0)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "\n",
        "    if df_train_label[df_train_label['image_name'] == name_of_image]['label'].count() == 1:\n",
        "      lable = df_train_label[df_train_label['image_name'] == name_of_image].iloc[0]['label']\n",
        "     \n",
        "      if (lable == 'real'):\n",
        "        x_train_real[per].append(img)\n",
        "      else:\n",
        "        x_train_forged[per].append(img)\n",
        "  \n",
        "  print(f'number of real Signature train : {len(x_train_real[per])}')\n",
        "  print(f'number of forged Signature train : {len(x_train_forged[per])}')\n",
        "  \n",
        "## read test image and divided into real and fake\n",
        "  for image in test_image:\n",
        "    name_of_image = image.split('/Test/')[1]\n",
        "    img = cv2.imread(image,0)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    if df_test_label[df_test_label['image_name'] == name_of_image]['label'].count() == 1:\n",
        "      lable = df_test_label[df_test_label['image_name'] == name_of_image].iloc[0]['label']\n",
        "     \n",
        "      if (lable == 'real'):\n",
        "        x_test_real[per].append(img)\n",
        "      else:\n",
        "        x_test_forged[per].append(img)\n",
        "\n",
        "  print(f'number of real Signature test : {len(x_test_real[per])}')\n",
        "  print(f'number of forged Signature test : {len(x_test_forged[per])}')\n",
        "\n",
        "  \n",
        "  print('======================================================================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkoGm-vjvA5M"
      },
      "outputs": [],
      "source": [
        "def make_Triplets(real_image,forged_image):\n",
        "  size_h = 128\n",
        "  size_w =128\n",
        "  tiplets =[]\n",
        " \n",
        " \n",
        "  for key,images in real_image.items(): \n",
        "\n",
        "    lable_person = key\n",
        "\n",
        "    for i in range(len(images)):\n",
        "      for j in range(i+1,len(images)):\n",
        "\n",
        "        AnchorImage = images[i]\n",
        "        AnchorImage = cv2.resize(AnchorImage,(size_h,size_w))\n",
        "      \n",
        "        PostiveImage = images[j]\n",
        "        PostiveImage =cv2.resize(PostiveImage,(size_h,size_w))\n",
        "        \n",
        "        NegativeImage =np.random.choice(x_train_forged[lable_person])\n",
        "        NegativeImage = cv2.resize(NegativeImage,(size_h,size_w))\n",
        "       \n",
        "        tiplets.append((AnchorImage,PostiveImage,NegativeImage))\n",
        "\n",
        "  return tiplets  \n",
        "\n",
        "pairImage_train =  make_Triplets(x_train_real,x_train_forged)\n",
        "pairImage_test =   make_Triplets(x_test_real,x_test_forged)\n",
        "\n",
        "print(f'length of triple train : {len(pairImage_train)}')\n",
        "print(f'length of triple train : {len(pairImage_test)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(triplet_list, batch_size=16, preprocess=True):\n",
        "    batch_steps = len(triplet_list)//batch_size\n",
        "    \n",
        "    for i in range(batch_steps+1):\n",
        "        anchor   = []\n",
        "        positive = []\n",
        "        negative = []\n",
        "        \n",
        "        j = i*batch_size\n",
        "        while j<(i+1)*batch_size and j<len(triplet_list):\n",
        "            a, p, n = triplet_list[j]\n",
        "            anchor.append(a)\n",
        "            positive.append(p)\n",
        "            negative.append(n)\n",
        "            j+=1\n",
        "            \n",
        "        anchor = np.array(anchor)\n",
        "        positive = np.array(positive)\n",
        "        negative = np.array(negative)\n",
        "        \n",
        "        if preprocess:\n",
        "            anchor = preprocess_input(anchor)\n",
        "            positive = preprocess_input(positive)\n",
        "            negative = preprocess_input(negative)\n",
        "        \n",
        "        yield ([anchor, positive, negative])"
      ],
      "metadata": {
        "id": "2PuRHvz8jkQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = 6\n",
        "\n",
        "f, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n",
        "\n",
        "for x in get_batch(pairImage_train, batch_size=num_plots, preprocess=False):\n",
        "    a,p,n = x\n",
        "    for i in range(num_plots):\n",
        "        axes[i, 0].imshow(a[i])\n",
        "        axes[i, 1].imshow(p[i])\n",
        "        axes[i, 2].imshow(n[i])\n",
        "        \n",
        "        i+=1\n",
        "    break"
      ],
      "metadata": {
        "id": "pImTt65TkzR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder(input_shape):\n",
        "    \n",
        "## pre - Trained Model Xception \n",
        "    pretrained_model = Xception(\n",
        "        input_shape=input_shape,\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        pooling='avg',\n",
        "    )\n",
        "\n",
        "## make all layers not train except last 27 layer of Xception\n",
        "    for i in range(len(pretrained_model.layers)-27):\n",
        "        pretrained_model.layers[i].trainable = False\n",
        "\n",
        "## make layers after Xception that get from it emadding \n",
        "    encode_model = Sequential([\n",
        "        pretrained_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
        "    ], name=\"Encode_Model\")\n",
        "    \n",
        "    return encode_model"
      ],
      "metadata": {
        "id": "dxwLkAtymV-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceLayer(layers.Layer):\n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)"
      ],
      "metadata": {
        "id": "axRAhnpOmcwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_siamese_network(input_shape = (128, 128, 3)):\n",
        "    encoder = get_encoder(input_shape)\n",
        "    \n",
        "    # Input Layers for the images\n",
        "    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n",
        "    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n",
        "    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n",
        "    \n",
        "    ## Generate the encodings (feature vectors) for the images\n",
        "    encoded_a = encoder(anchor_input)\n",
        "    encoded_p = encoder(positive_input)\n",
        "    encoded_n = encoder(negative_input)\n",
        "    \n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    distances = DistanceLayer()(\n",
        "       encoded_a,\n",
        "       encoded_p,\n",
        "       encoded_n,\n",
        "    )\n",
        "    \n",
        "    # Creating the Model\n",
        "    siamese_network = Model(\n",
        "        inputs  = [anchor_input, positive_input, negative_input],\n",
        "        outputs = distances,\n",
        "        name = \"Siamese_Network\"\n",
        "    )\n",
        "    return siamese_network\n",
        "\n",
        "siamese_network = get_siamese_network()\n",
        "siamese_network.summary()"
      ],
      "metadata": {
        "id": "pDlTOoMamV8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(siamese_network, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "JFFL9W1DmV5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseModel(Model):\n",
        "    # Builds a Siamese model based on a base-model\n",
        "    def __init__(self, siamese_network, margin=1.0):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        \n",
        "        self.margin = margin\n",
        "        self.siamese_network = siamese_network\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
        "        \n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "        \n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        # Get the two distances from the network, then compute the triplet loss\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics so the reset_states() can be called automatically.\n",
        "        return [self.loss_tracker]"
      ],
      "metadata": {
        "id": "6zwo2z07mV3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n",
        "siamese_model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "Rj3WEgYtmV01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_on_triplets(batch_size = 256):\n",
        "    pos_scores, neg_scores = [], []\n",
        "\n",
        "    for data in get_batch(pairImage_test, batch_size=batch_size):\n",
        "        prediction = siamese_model.predict(data)\n",
        "        pos_scores += list(prediction[0])\n",
        "        neg_scores += list(prediction[1])\n",
        "    \n",
        "    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n",
        "\n",
        "    ap_mean = np.mean(pos_scores)\n",
        "    an_mean = np.mean(neg_scores)\n",
        "    \n",
        "    ap_stds = np.std(pos_scores)\n",
        "    an_stds = np.std(neg_scores)\n",
        "    \n",
        "    print(f\"Accuracy on test = {accuracy:.5f}\")\n",
        "    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)"
      ],
      "metadata": {
        "id": "RjUCnUjcmVyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_all = False\n",
        "epochs = 6\n",
        "batch_size = 256\n",
        "\n",
        "max_acc = 0\n",
        "train_loss = []\n",
        "test_metrics = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    t = time.time()\n",
        "    \n",
        "    # Training the model on train data\n",
        "    epoch_loss = []\n",
        "    for data in get_batch(pairImage_train, batch_size=batch_size):\n",
        "        loss = siamese_model.train_on_batch(data)\n",
        "        epoch_loss.append(loss)\n",
        "    epoch_loss = sum(epoch_loss)/len(epoch_loss)\n",
        "    train_loss.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n",
        "    print(f\"Loss on train    = {epoch_loss:.5f}\")\n",
        "    \n",
        "    # Testing the model on test data\n",
        "    metric = test_on_triplets(batch_size=batch_size)\n",
        "    test_metrics.append(metric)\n",
        "    accuracy = metric[0]\n",
        "    \n",
        "    # Saving the model weights\n",
        "    if save_all or accuracy>=max_acc:\n",
        "        siamese_model.save_weights(\"siamese_model\")\n",
        "        max_acc = accuracy\n",
        "\n",
        "# Saving the model after all epochs run\n",
        "siamese_model.save_weights(\"siamese_model-final\")"
      ],
      "metadata": {
        "id": "EZYz-Z6FmVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_encoder(model):\n",
        "    encoder = get_encoder((128, 128, 3))\n",
        "    i=0\n",
        "    for e_layer in model.layers[0].layers[3].layers:\n",
        "        layer_weight = e_layer.get_weights()\n",
        "        encoder.layers[i].set_weights(layer_weight)\n",
        "        i+=1\n",
        "    return encoder\n",
        "\n",
        "encoder = extract_encoder(siamese_model)\n",
        "encoder.save_weights(\"encoder\")\n",
        "#encoder.save(\"/content/drive/MyDrive/cv_data/model.h5\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "SCNfNWNCmVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_images(Image1,Image2, threshold=1.3):\n",
        "    # Getting the encodings for the passed faces\n",
        "    embedding1 = encoder.predict(Image1)\n",
        "    embedding2 = encoder.predict(Image2)\n",
        "    \n",
        "    distance = np.sum(np.square(embedding1-embedding2), axis=-1)\n",
        "    prediction = np.where(distance<=threshold, 0, 1)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "B_vFWJMbmVqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelMetrics(pos_list, neg_list):\n",
        "    true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n",
        "    pred = np.append(pos_list, neg_list)\n",
        "    \n",
        "    # Compute and print the accuracy\n",
        "    print(f\"\\nAccuracy of model: {accuracy_score(true, pred)*100}%\\n\")\n",
        "    \n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(true, pred)\n",
        "\n",
        "    categories  = ['Similar','Different']\n",
        "    names = ['True Similar','False Similar', 'False Different','True Different']\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
        "\n",
        "\n",
        "pos_list = np.array([])\n",
        "neg_list = np.array([])\n",
        "\n",
        "for data in get_batch(pairImage_test, batch_size=len(pairImage_test)):\n",
        "    print('a')\n",
        "    a, p, n = data\n",
        "    pos_list = np.append(pos_list, classify_images(a, p))\n",
        "    neg_list = np.append(neg_list, classify_images(a, n))\n",
        "    break\n",
        "\n",
        "ModelMetrics(pos_list, neg_list)"
      ],
      "metadata": {
        "id": "dX1Y0CqgmVnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwTK9UFGmVk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hr7XybOYmVP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PKpNSwQvEtc"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqlbx5_Gh96u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NEQ3edLTciAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}